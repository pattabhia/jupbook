{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning HaiJava-Surgeon with Dynamic Prompt Templates\n",
    "\n",
    "This notebook implements fine-tuning for the `Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1` model using the `code_x_glue_cc_code_refinement` dataset from Hugging Face.\n",
    "\n",
    "## Key Features:\n",
    "- **Dataset**: CodeXGLUE Code Refinement (small subset)\n",
    "- **Dynamic Prompts**: 5 different prompt templates randomly selected during preprocessing\n",
    "- **Training**: 3 epochs with LoRA fine-tuning\n",
    "- **Format**: Chat-based format with system/user/assistant roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.5)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets peft accelerate bitsandbytes torch\n",
    "!pip install -U datasets\n",
    "!pip install -U \\\n",
    "  transformers \\\n",
    "  datasets \\\n",
    "  accelerate \\\n",
    "  peft \\\n",
    "  bitsandbytes \\\n",
    "  sentencepiece \\\n",
    "  safetensors\n",
    "!pip install -U hf_transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from typing import Dict, List\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1\n",
      "  Dataset: code_x_glue_cc_code_refinement (small subset, train split)\n",
      "  Epochs: 1\n",
      "  Batch Size: 1\n",
      "  Learning Rate: 0.0002\n",
      "  Output Directory: ./models/haijava_dynamic_prompts\n"
     ]
    }
   ],
   "source": [
    "# Model and dataset configuration\n",
    "MODEL_NAME = \"Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1\"\n",
    "DATASET_NAME = \"code_x_glue_cc_code_refinement\"\n",
    "DATASET_SUBSET = \"small\"  # Using SMALL subset (â‰¤50 tokens)\n",
    "DATASET_SPLIT = \"train\"   # Using TRAIN split only\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 750\n",
    "OUTPUT_DIR = \"./models/haijava_dynamic_prompts\"\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Dataset: {DATASET_NAME} ({DATASET_SUBSET} subset, {DATASET_SPLIT} split)\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define 5 Dynamic Prompt Templates\n",
    "\n",
    "These templates will be randomly selected during preprocessing for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defined 5 prompt templates:\n",
      "\n",
      "1. DETAILED_EXPERT\n",
      "   System: You are a Java code fixing assistant.\n",
      "   User: You are a Java expert. Fix the following buggy Java code. Correct syntax errors, logic bugs, runtime...\n",
      "\n",
      "2. SIMPLE\n",
      "   System: You are a Java code fixing assistant.\n",
      "   User: Fix the following Java code.\n",
      "\n",
      "{buggy_code}...\n",
      "\n",
      "3. MINIMAL\n",
      "   System: You are a Java code fixing assistant.\n",
      "   User: {buggy_code}...\n",
      "\n",
      "4. STRUCTURED\n",
      "   System: You are a Java code fixing assistant.\n",
      "   User: ### Buggy Code\n",
      "{buggy_code}\n",
      "\n",
      "### Corrected Code...\n",
      "\n",
      "5. LINE_BY_LINE\n",
      "   System: You are a Java code fixing assistant.\n",
      "   User: Analyze and fix the following buggy Java code line by line:\n",
      "\n",
      "{buggy_code}\n",
      "\n",
      "Provide the corrected ver...\n"
     ]
    }
   ],
   "source": [
    "def get_prompt_templates() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Returns 5 different prompt templates for dynamic selection.\n",
    "    Each template follows the chat format with system/user/assistant roles.\n",
    "    \"\"\"\n",
    "    templates = [\n",
    "        # Template 1: Detailed expert instruction\n",
    "        {\n",
    "            \"name\": \"detailed_expert\",\n",
    "            \"system\": \"You are a Java code fixing assistant.\",\n",
    "            \"user_template\": \"You are a Java expert. Fix the following buggy Java code. Correct syntax errors, logic bugs, runtime errors, and code quality issues. Output ONLY the corrected code.\\n\\n{buggy_code}\"\n",
    "        },\n",
    "        # Template 2: Simple instruction\n",
    "        {\n",
    "            \"name\": \"simple\",\n",
    "            \"system\": \"You are a Java code fixing assistant.\",\n",
    "            \"user_template\": \"Fix the following Java code.\\n\\n{buggy_code}\"\n",
    "        },\n",
    "        # Template 3: Minimal (code only)\n",
    "        {\n",
    "            \"name\": \"minimal\",\n",
    "            \"system\": \"You are a Java code fixing assistant.\",\n",
    "            \"user_template\": \"{buggy_code}\"\n",
    "        },\n",
    "        # Template 4: Structured format\n",
    "        {\n",
    "            \"name\": \"structured\",\n",
    "            \"system\": \"You are a Java code fixing assistant.\",\n",
    "            \"user_template\": \"### Buggy Code\\n{buggy_code}\\n\\n### Corrected Code\"\n",
    "        },\n",
    "        # Template 5: Line-by-line format\n",
    "        {\n",
    "            \"name\": \"line_by_line\",\n",
    "            \"system\": \"You are a Java code fixing assistant.\",\n",
    "            \"user_template\": \"Analyze and fix the following buggy Java code line by line:\\n\\n{buggy_code}\\n\\nProvide the corrected version:\"\n",
    "        }\n",
    "    ]\n",
    "    return templates\n",
    "\n",
    "# Display templates\n",
    "templates = get_prompt_templates()\n",
    "print(f\"\\nDefined {len(templates)} prompt templates:\")\n",
    "for i, template in enumerate(templates, 1):\n",
    "    print(f\"\\n{i}. {template['name'].upper()}\")\n",
    "    print(f\"   System: {template['system']}\")\n",
    "    print(f\"   User: {template['user_template'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "Loading the CodeXGLUE Code Refinement dataset (small subset, train split only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: code_x_glue_cc_code_refinement (small subset)...\n",
      "Using split: train\n",
      "\n",
      "âœ… Dataset loaded successfully!\n",
      "   Total samples: 46680\n",
      "   Features: {'id': Value('int32'), 'buggy': Value('string'), 'fixed': Value('string')}\n",
      "\n",
      "ðŸ“‹ Sample from dataset:\n",
      "\n",
      "Buggy code (first 200 chars):\n",
      "public java.lang.String METHOD_1 ( ) { return new TYPE_1 ( STRING_1 ) . format ( VAR_1 [ ( ( VAR_1 . length ) - 1 ) ] . getTime ( ) ) ; } \n",
      "...\n",
      "\n",
      "Fixed code (first 200 chars):\n",
      "public java.lang.String METHOD_1 ( ) { return new TYPE_1 ( STRING_1 ) . format ( VAR_1 [ ( ( type ) - 1 ) ] . getTime ( ) ) ; } \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset: {DATASET_NAME} ({DATASET_SUBSET} subset)...\")\n",
    "print(f\"Using split: {DATASET_SPLIT}\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    DATASET_SUBSET,\n",
    "    split=DATASET_SPLIT\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"   Total samples: {len(dataset)}\")\n",
    "print(f\"   Features: {dataset.features}\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\nðŸ“‹ Sample from dataset:\")\n",
    "sample = dataset[0]\n",
    "print(f\"\\nBuggy code (first 200 chars):\\n{sample['buggy'][:200]}...\")\n",
    "print(f\"\\nFixed code (first 200 chars):\\n{sample['fixed'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1...\n",
      "âœ… Tokenizer loaded successfully!\n",
      "   Vocab size: 151665\n",
      "   EOS token: <|im_end|> (ID: 151645)\n",
      "   PAD token: <|endoftext|> (ID: 151643)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad_token to eos_token: {tokenizer.eos_token}\")\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded successfully!\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"   PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca61b93f25f648c4bf829f57edbc45e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "   Model type: qwen2\n",
      "   Hidden size: 3584\n",
      "   Num layers: 28\n",
      "   Num attention heads: 28\n",
      "\n",
      "   Total parameters: 7,615,616,512\n",
      "   Trainable parameters (before LoRA): 7,615,616,512\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading model: {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Load model in 8-bit for memory efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"   Model type: {model.config.model_type}\")\n",
    "print(f\"   Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"   Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"   Num attention heads: {model.config.num_attention_heads}\")\n",
    "\n",
    "# Calculate model size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters (before LoRA): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:78: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRA applied successfully!\n",
      "\n",
      "trainable params: 161,480,704 || all params: 7,777,097,216 || trainable%: 2.0764\n",
      "\n",
      "Trainable parameters: 161,480,704 (2.08% of total)\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying LoRA configuration...\\n\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"âœ… LoRA applied successfully!\\n\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Calculate trainable percentage\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} ({trainable_percent:.2f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dynamic Preprocessing Function\n",
    "\n",
    "This function randomly selects one of the 5 prompt templates for each sample during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing function defined!\n",
      "\n",
      "This function will:\n",
      "  1. Randomly select one of 5 templates for each sample\n",
      "  2. Format the prompt using the selected template\n",
      "  3. Create chat messages (system/user/assistant)\n",
      "  4. Apply tokenizer's chat template\n",
      "  5. Tokenize and prepare for training\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Preprocess function that dynamically applies one of 5 prompt templates.\n",
    "    \n",
    "    Workflow:\n",
    "    1. For each sample, randomly select a template\n",
    "    2. Format the buggy code using the selected template\n",
    "    3. Create chat messages with system/user/assistant roles\n",
    "    4. Apply tokenizer's chat template\n",
    "    5. Tokenize the result\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples from dataset\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs ready for training\n",
    "    \"\"\"\n",
    "    templates = get_prompt_templates()\n",
    "    \n",
    "    formatted_texts = []\n",
    "    prompt_lengths = []\n",
    "    \n",
    "    # Process each example in  =the batch\n",
    "    # for buggy_code, fixed_code in zip(examples['buggy'], examples['fixed']):\n",
    "    #     # Randomly select a template\n",
    "    #     template = random.choice(templates)\n",
    "        \n",
    "    #     # Format the user message with the selected template\n",
    "    #     user_content = template['user_template'].format(buggy_code=buggy_code)\n",
    "        \n",
    "    #     # Create chat messages\n",
    "    #     messages = [\n",
    "    #         {\"role\": \"system\", \"content\": template['system']},\n",
    "    #         {\"role\": \"user\", \"content\": user_content},\n",
    "    #         {\"role\": \"assistant\", \"content\": fixed_code}\n",
    "    #     ]\n",
    "        \n",
    "    #     # Apply chat template\n",
    "    #     formatted_text = tokenizer.apply_chat_template(\n",
    "    #         messages,\n",
    "    #         tokenize=False,\n",
    "    #         add_generation_prompt=False\n",
    "    #     )\n",
    "        \n",
    "    #     formatted_texts.append(formatted_text)\n",
    "\n",
    "\n",
    "    for buggy_code, fixed_code in zip(examples[\"buggy\"], examples[\"fixed\"]):\n",
    "        template = random.choice(templates)\n",
    "    \n",
    "        # Build prompt (NO chat roles)\n",
    "        prompt = template[\"user_template\"].format(buggy_code=buggy_code)\n",
    "    \n",
    "        # Full causal text\n",
    "        full_text = prompt + \"\\n\" + fixed_code\n",
    "    \n",
    "        formatted_texts.append(full_text)\n",
    "    \n",
    "        # Track prompt length for label masking\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt,\n",
    "            add_special_tokens=False\n",
    "        )[\"input_ids\"]\n",
    "        prompt_lengths.append(len(prompt_ids))\n",
    "\n",
    "    \n",
    "    # Tokenize all formatted texts\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,  # Will be handled by data collator\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    labels = []\n",
    "\n",
    "    for input_ids, prompt_len in zip(tokenized[\"input_ids\"], prompt_lengths):\n",
    "        lbl = input_ids.copy()\n",
    "        lbl[:min(prompt_len, len(lbl))] = [-100] * min(prompt_len, len(lbl))\n",
    "        labels.append(lbl)\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"âœ… Preprocessing function defined!\")\n",
    "print(\"\\nThis function will:\")\n",
    "print(\"  1. Randomly select one of 5 templates for each sample\")\n",
    "print(\"  2. Format the prompt using the selected template\")\n",
    "print(\"  3. Create chat messages (system/user/assistant)\")\n",
    "print(\"  4. Apply tokenizer's chat template\")\n",
    "print(\"  5. Tokenize and prepare for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Preprocessing with Examples\n",
    "\n",
    "Let's test the preprocessing function to see how different templates are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing with 3 examples...\n",
      "\n",
      "================================================================================\n",
      "Example 1\n",
      "================================================================================\n",
      "\n",
      "Formatted text (first 500 chars):\n",
      "You are a Java expert. Fix the following buggy Java code. Correct syntax errors, logic bugs, runtime errors, and code quality issues. Output ONLY the corrected code.\n",
      "\n",
      "public java.lang.String METHOD_1 ( ) { return new TYPE_1 ( STRING_1 ) . format ( VAR_1 [ ( ( VAR_1 . length ) - 1 ) ] . getTime ( ) ) ; } \n",
      "\n",
      "public java.lang.String METHOD_1 ( ) { return new TYPE_1 ( STRING_1 ) . format ( VAR_1 [ ( ( type ) - 1 ) ] . getTime ( ) ) ; } \n",
      "...\n",
      "\n",
      "Token count: 126\n",
      "\n",
      "================================================================================\n",
      "Example 2\n",
      "================================================================================\n",
      "\n",
      "Formatted text (first 500 chars):\n",
      "You are a Java expert. Fix the following buggy Java code. Correct syntax errors, logic bugs, runtime errors, and code quality issues. Output ONLY the corrected code.\n",
      "\n",
      "public boolean METHOD_1 ( java.lang.String name ) { TYPE_1 VAR_1 = TYPE_1 . METHOD_2 ( VAR_2 ) ; return ( ! ( METHOD_3 ( name ) ) ) && ( VAR_1 . contains ( name ) ) ; } \n",
      "\n",
      "public boolean METHOD_1 ( java.lang.String name ) { return ( ! ( METHOD_3 ( name ) ) ) && ( TYPE_1 . METHOD_2 ( VAR_2 ) . contains ( name ) ) ; } \n",
      "...\n",
      "\n",
      "Token count: 139\n",
      "\n",
      "================================================================================\n",
      "Example 3\n",
      "================================================================================\n",
      "\n",
      "Formatted text (first 500 chars):\n",
      "public char METHOD_1 ( java.lang.String VAR_1 , java.lang.String name ) { return null ; } \n",
      "\n",
      "public char METHOD_1 ( java.lang.String VAR_1 , java.lang.String name ) { return 0 ; } \n",
      "...\n",
      "\n",
      "Token count: 49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing preprocessing with 3 examples...\\n\")\n",
    "\n",
    "# Get a small sample\n",
    "test_sample = dataset.select(range(3))\n",
    "\n",
    "# Process each sample individually to see different templates\n",
    "for i in range(3):\n",
    "    sample = test_sample[i]\n",
    "    \n",
    "    # Create a single-item batch\n",
    "    batch = {\n",
    "        'buggy': [sample['buggy']],\n",
    "        'fixed': [sample['fixed']]\n",
    "    }\n",
    "    \n",
    "    # Process\n",
    "    processed = preprocess_function(batch)\n",
    "    \n",
    "    # Decode to see the formatted text\n",
    "    \n",
    "    formatted_text = tokenizer.decode(processed['input_ids'][0], skip_special_tokens=False)\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nFormatted text (first 500 chars):\\n{formatted_text[:500]}...\")\n",
    "    print(f\"\\nToken count: {len(processed['input_ids'][0])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apply Preprocessing to Dataset\n",
    "\n",
    "Now we'll apply the preprocessing function to the entire dataset. Each sample will get a randomly selected template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing to dataset...\n",
      "This will randomly assign templates to each sample.\n",
      "\n",
      "\n",
      "âœ… Preprocessing complete!\n",
      "   Total samples: 46680\n",
      "   Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "\n",
      "Token length statistics:\n",
      "   Min: 18\n",
      "   Max: 219\n",
      "   Mean: 108.4\n",
      "   Median: 108\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying preprocessing to dataset...\")\n",
    "print(\"This will randomly assign templates to each sample.\\n\")\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Preprocessing complete!\")\n",
    "print(f\"   Total samples: {len(tokenized_dataset)}\")\n",
    "print(f\"   Features: {tokenized_dataset.features}\")\n",
    "\n",
    "# Show token length statistics\n",
    "token_lengths = [len(x) for x in tokenized_dataset['input_ids']]\n",
    "print(f\"\\nToken length statistics:\")\n",
    "print(f\"   Min: {min(token_lengths)}\")\n",
    "print(f\"   Max: {max(token_lengths)}\")\n",
    "print(f\"   Mean: {sum(token_lengths) / len(token_lengths):.1f}\")\n",
    "print(f\"   Median: {sorted(token_lengths)[len(token_lengths)//2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split (90/10)...\n",
      "\n",
      "âœ… Split complete!\n",
      "   Training samples: 42012\n",
      "   Validation samples: 4668\n",
      "   Split ratio: 90.0% train, 10.0% validation\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train/validation split (90/10)...\\n\")\n",
    "\n",
    "# Split dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"âœ… Split complete!\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"   Split ratio: {len(train_dataset)/len(tokenized_dataset)*100:.1f}% train, {len(eval_dataset)/len(tokenized_dataset)*100:.1f}% validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Setup Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training configuration...\n",
      "\n",
      "Training configuration:\n",
      "   Total training samples: 42012\n",
      "   Steps per epoch: 42012\n",
      "   Total training steps: 42012\n",
      "   Evaluation steps: 10503\n",
      "   Save steps: 21006\n",
      "\n",
      "âœ… Training arguments configured!\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up training configuration...\\n\")\n",
    "\n",
    "# Calculate training steps\n",
    "num_train_samples = len(train_dataset)\n",
    "steps_per_epoch = num_train_samples // BATCH_SIZE\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "eval_steps = steps_per_epoch // 4  # Evaluate 4 times per epoch\n",
    "save_steps = steps_per_epoch // 2  # Save 2 times per epoch\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"   Total training samples: {num_train_samples}\")\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "print(f\"   Evaluation steps: {eval_steps}\")\n",
    "print(f\"   Save steps: {save_steps}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    eval_steps=5251,\n",
    "    save_steps=10502,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",  # Change to \"tensorboard\" or \"wandb\" if desired\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLMWithPadding:\n",
    "    tokenizer: Any\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract labels\n",
    "        labels = [f.pop(\"labels\") for f in features]\n",
    "\n",
    "        # Pad inputs\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Pad labels manually\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        padded_labels = [\n",
    "            lbl + [self.label_pad_token_id] * (max_len - len(lbl))\n",
    "            for lbl in labels\n",
    "        ]\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Data Collator and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data collator and trainer...\n",
      "\n",
      "âœ… Trainer created successfully!\n",
      "\n",
      "Training workflow:\n",
      "  1. Trainer pulls a batch from train_dataset\n",
      "  2. Data collator pads the batch\n",
      "  3. Model processes the batch (forward pass)\n",
      "  4. Loss is calculated\n",
      "  5. Gradients are computed (backward pass)\n",
      "  6. Optimizer updates LoRA parameters\n",
      "  7. Repeat for all batches and epochs\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating data collator and trainer...\\n\")\n",
    "from transformers import DataCollatorWithPadding\n",
    "# Data collator for language modeling\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False  # Causal LM, not masked LM\n",
    "# )\n",
    "data_collator = DataCollatorForCausalLMWithPadding(\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer created successfully!\")\n",
    "print(\"\\nTraining workflow:\")\n",
    "print(\"  1. Trainer pulls a batch from train_dataset\")\n",
    "print(\"  2. Data collator pads the batch\")\n",
    "print(\"  3. Model processes the batch (forward pass)\")\n",
    "print(\"  4. Loss is calculated\")\n",
    "print(\"  5. Gradients are computed (backward pass)\")\n",
    "print(\"  6. Optimizer updates LoRA parameters\")\n",
    "print(\"  7. Repeat for all batches and epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Start Training\n",
    "\n",
    "**Note**: This will take several hours depending on your hardware. The training will run for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      "\n",
      "Model: Haiintel/HaiJava-Surgeon-Qwen2.5-Coder-7B-SFT-v1\n",
      "Dataset: code_x_glue_cc_code_refinement (small subset, train split)\n",
      "Training samples: 42012\n",
      "Validation samples: 4668\n",
      "Epochs: 1\n",
      "Batch size: 1\n",
      "Learning rate: 0.0002\n",
      "\n",
      "Dynamic prompts: 5 templates randomly selected per sample\n",
      "\n",
      "Estimated time: Several hours (depends on hardware)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME} ({DATASET_SUBSET} subset, {DATASET_SPLIT} split)\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nDynamic prompts: 5 templates randomly selected per sample\")\n",
    "print(f\"\\nEstimated time: Several hours (depends on hardware)\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining metrics:\")\n",
    "print(f\"   Final train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Total steps: {train_result.global_step}\")\n",
    "print(f\"   Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on validation set...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"âœ… Evaluation complete!\")\n",
    "print(f\"\\nValidation metrics:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving fine-tuned model...\\n\")\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = f\"{OUTPUT_DIR}/final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"âœ… Model saved to: {final_model_path}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "import os\n",
    "for file in os.listdir(final_model_path):\n",
    "    file_path = os.path.join(final_model_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"   {file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test the Fine-tuned Model\n",
    "\n",
    "Let's test the model with a sample buggy code to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing the fine-tuned model...\\n\")\n",
    "\n",
    "# Get a test sample from the dataset\n",
    "test_sample = dataset[100]  # Use a different sample than training\n",
    "buggy_code = test_sample['buggy']\n",
    "expected_fixed = test_sample['fixed']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST SAMPLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ› Buggy Code:\\n{buggy_code}\")\n",
    "print(f\"\\nâœ… Expected Fixed Code:\\n{expected_fixed}\")\n",
    "\n",
    "# Create a prompt using Template 1 (detailed expert)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Java code fixing assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"You are a Java expert. Fix the following buggy Java code. Correct syntax errors, logic bugs, runtime errors, and code quality issues. Output ONLY the corrected code.\\n\\n{buggy_code}\"}\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "print(\"\\nðŸ¤– Generating fix...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract just the assistant's response\n",
    "if \"assistant\" in generated_text:\n",
    "    model_output = generated_text.split(\"assistant\")[-1].strip()\n",
    "else:\n",
    "    model_output = generated_text\n",
    "\n",
    "print(f\"\\nðŸ”§ Model's Fixed Code:\\n{model_output}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Test with Multiple Templates\n",
    "\n",
    "Let's test the same buggy code with different prompt templates to see how the model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing with all 5 prompt templates...\\n\")\n",
    "\n",
    "templates = get_prompt_templates()\n",
    "test_sample = dataset[150]\n",
    "buggy_code = test_sample['buggy']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Buggy Code:\\n{buggy_code}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, template in enumerate(templates, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Template {i}: {template['name'].upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create messages\n",
    "    user_content = template['user_template'].format(buggy_code=buggy_code)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": template['system']},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    # Format and tokenize\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"assistant\" in generated_text:\n",
    "        model_output = generated_text.split(\"assistant\")[-1].strip()\n",
    "    else:\n",
    "        model_output = generated_text\n",
    "    \n",
    "    print(f\"\\nModel Output (first 300 chars):\\n{model_output[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Model: {MODEL_NAME}\")\n",
    "print(f\"âœ… Dataset: {DATASET_NAME} ({DATASET_SUBSET} subset, {DATASET_SPLIT} split)\")\n",
    "print(f\"âœ… Training samples: {len(train_dataset)}\")\n",
    "print(f\"âœ… Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"âœ… Epochs completed: {NUM_EPOCHS}\")\n",
    "print(f\"âœ… Dynamic prompt templates: 5 templates randomly selected\")\n",
    "print(f\"âœ… Model saved to: {final_model_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. **Evaluate on Test Set**: Load the test split and evaluate model performance\n",
    "\n",
    "2. **Merge LoRA Adapters**: Merge the LoRA adapters with the base model for deployment\n",
    "   ```python\n",
    "   from peft import PeftModel\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "   model = PeftModel.from_pretrained(base_model, final_model_path)\n",
    "   merged_model = model.merge_and_unload()\n",
    "   merged_model.save_pretrained(\"./models/haijava_merged\")\n",
    "   ```\n",
    "\n",
    "3. **Upload to Hugging Face Hub**: Share your fine-tuned model\n",
    "   ```python\n",
    "   merged_model.push_to_hub(\"your-username/haijava-surgeon-finetuned\")\n",
    "   tokenizer.push_to_hub(\"your-username/haijava-surgeon-finetuned\")\n",
    "   ```\n",
    "\n",
    "4. **Run Comprehensive Evaluation**: Test on various Java bug-fixing benchmarks\n",
    "\n",
    "5. **Analyze Template Performance**: Check which templates led to better results\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING COMPLETE! ðŸŽ‰\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
